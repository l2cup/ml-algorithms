# -*- coding: utf-8 -*-
"""3c.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KFF2O9VxE9oOilm3Uy8ohQpzZMockmee
"""

import os
# Resetting the google colab runtime
#os.kill(os.getpid(), 9)

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
# %matplotlib inline

class KNN:
  
  def __init__(self, nb_features, nb_classes, data, k):
    self.nb_features = nb_features
    self.nb_classes = nb_classes
    self.data = data
    self.k = k
    
    # Gradimo model, X je matrica podataka a Q je vektor koji predstavlja upit.
    self.X = tf.placeholder(shape=(None, nb_features), dtype=tf.float32)
    self.Y = tf.placeholder(shape=(None), dtype=tf.int32)
    self.Q = tf.placeholder(shape=(nb_features), dtype=tf.float32)
    
    # Racunamo kvadriranu euklidsku udaljenost i uzimamo minimalnih k.
    self.dists = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(self.X, self.Q)), 
                                       axis=1)) 
                                  
    _, self.idxs = tf.nn.top_k(-self.dists, self.k)  
    
    self.classes = tf.gather(self.Y, self.idxs)
    self.dists = tf.gather(self.dists, self.idxs)

    # Svaki red mnozimo svojim glasom i sabiramo glasove po kolonama.
    # Posto nam zadatak samo zahteva weightless, filujemo [k] sa 1/k i odmah
    # potom ga reshapujemo.
    w = tf.fill([k], 1/k)
    w_col = tf.reshape(w, (k, 1))
    one_hot = tf.one_hot(self.classes, self.nb_classes)
    self.scores = tf.reduce_sum(w_col * one_hot, axis=0)
    
    # Klasa sa najvise glasova je hipoteza.
    self.hyp = tf.argmax(self.scores)
  
  # Ako imamo odgovore za upit racunamo i accuracy.
  def predict(self, query_data):
    
    with tf.Session() as sess:
      sess.run(tf.global_variables_initializer())
      
      nb_queries = query_data['x'].shape[0]
      
      matches = 0

      for i in range(nb_queries):
        hyp_val = sess.run(self.hyp, feed_dict = {self.X: self.data['x'], 
                                                  self.Y: self.data['y'], 
                                                  self.Q: query_data['x'][i]})
        if query_data['y'] is not None:
          actual = query_data['y'][i]
          match = (hyp_val == actual)
          if match:
            matches += 1
      
      accuracy = matches / nb_queries
      print("%d matches out of %d examples" % (matches, nb_queries))
      return accuracy

nb_features = 8
nb_classes = 2

filename = "Prostate_Cancer.csv"
all_data = np.loadtxt(filename, delimiter=",", dtype="str", skiprows=1)


# Za podatke koje nisu dostupni uzimamo mean i punimo te podatke
for arr in all_data:
  non_NA = list(map(lambda x: x.astype(float),filter(lambda x: x not in {"NA", "M","B"}, arr)))
  mean = sum(non_NA) / len(non_NA)
  for i in range(0, len(arr)):
    if arr[i] == "NA":
      arr[i] = mean

key_val = {"M": 0, "B": 1}

data = dict()

data["y"] = np.array([(key_val[col[1]]) for col in all_data]).astype(float)
data["x"] = np.array([(col[2], col[3], col[4], col[5], col[6], col[7], col[8], col[9]) for col in all_data]).astype(float)

# Mesanje pomocu random indexa
nb_samples = data["x"].shape[0]
indices = np.random.permutation(nb_samples)
data["x"] = data["x"][indices]
data["y"] = data["y"][indices]

# Deljenje u train i test podatke
test_data = dict()
train_data = dict()

train_data_len = int(0.8 * nb_samples)

train_data["x"] = data["x"][:train_data_len]
train_data["y"] = data["y"][:train_data_len]

test_data["x"] = data["x"][train_data_len:]
test_data["y"] = data["y"][train_data_len:]

tf.reset_default_graph()
accuracies = []
for k in range(1, 16):
  knn = KNN(nb_features, nb_classes, train_data, k)
  accuracy = knn.predict(query_data = {"x" : test_data["x"], "y" : test_data["y"]})
  print('For k of %d acurracy is %f' % (k, accuracy))
  accuracies.append(accuracy)

fig = plt.figure(figsize = (5,5))
ax = fig.add_subplot(111)
ax.set_ylabel("Accuracy")
ax.set_xlabel("K value")
ax.plot([x for x in range(1,16)], accuracies, color="red", marker="o", ms=3)

# Ovde se dobijaju otprilike 10 do 15 % bolje vrednosti od toga kada imamo samo 4 feature-a.
# U sustini to znaci da korelacija izmedju feature-a i klase nije prevelika
# za poslednja 4 feature-a, vec su feature-i koji najvise uticu na klasifikaciju
# prva 4 feature-a.
# Za svako k imamo otprilike iste vrednosti, ali mozemo reci da k > 4 se
# pokazalo kao najbolji izbor